Want to try this project? Here's how:

Clone or download this repository
Install dependencies: pip install -r requirements.txt
Prepare your dataset (or use your own images)
Open the notebook: jupyter notebook Sign_Language_Detection.ipynb
Run all cells and train the model
Test with webcam for real-time detection!

No prior deep learning experience required - the notebook guides you through each step! ğŸ“š
ğŸŒŸ Features

Real-time hand gesture detection using MediaPipe
Deep learning CNN model for sign language classification
Detects 5 sign language gestures: Hello, Perfect, No, Yes, Thanks
Visual feedback with prediction display
High accuracy gesture recognition
Easy to use Jupyter Notebook interface

ğŸ› ï¸ Technologies Used

Python 3.x - Programming language
OpenCV - Image and video processing
MediaPipe - Hand landmark detection and tracking
TensorFlow/Keras - Deep learning model building and training
NumPy - Numerical computations and array operations
Pandas - Data manipulation and analysis
Matplotlib - Data visualization and plotting
scikit-learn - Machine learning utilities (train-test split, label encoding)

ğŸ“‹ Prerequisites

Python 3.7 or higher
Webcam (for real-time detection)
Jupyter Notebook

ğŸ”§ Installation

Clone the repository:

bashgit clone https://github.com/kanchan07r/sign-language-detection.git
cd sign-language-detection
Or Fork this repository to your own GitHub account and clone your fork.

Create a virtual environment (recommended):

bashpython -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

Install required dependencies:

bashpip install -r requirements.txt

Launch Jupyter Notebook:

bashjupyter notebook
ğŸ“¦ Dependencies
All required libraries are listed in requirements.txt:

opencv-python - Computer vision and image processing
numpy - Numerical operations
pandas - Data handling
matplotlib - Visualization
scikit-learn - Machine learning tools
tensorflow - Deep learning framework
mediapipe - Hand tracking and landmark detection
jupyter - Notebook interface

ğŸš€ Usage
Getting Started

Prepare your dataset (see Dataset Information section below)
Launch Jupyter Notebook:

bashjupyter notebook

Open Sign_Language_Detection.ipynb
Follow the notebook step-by-step

Notebook Workflow
The notebook is organized into clear sections:

ğŸ“š Import Libraries: Load all required packages
ğŸ“ Load Dataset: Load gesture images from the data/ folder
ğŸ”„ Data Preprocessing:

Resize images to uniform size
Normalize pixel values
Split into training and testing sets


ğŸ—ï¸ Build Model: Create the CNN architecture
ğŸ¯ Train Model: Train the model on your dataset
ğŸ“Š Evaluate Model: View accuracy, loss, and performance metrics
ğŸ¥ Real-time Detection: Use your webcam to detect gestures live!

Running Real-time Detection
After training the model:

Run the real-time detection cell
Allow browser camera access when prompted
Show gestures to the webcam
See predictions in real-time!
Press 'q' to quit

Customization
To add more gestures:

Create new folders in data/ with gesture names
Add images to those folders
Update the notebook to include new classes
Retrain the model

To improve accuracy:

Collect more training images (500+ per gesture recommended)
Use data augmentation techniques
Adjust model architecture (add more layers)
Increase training epochs

Quick Start
bash# Install dependencies
pip install -r requirements.txt

# Launch notebook
jupyter notebook Sign_Language_Detection.ipynb
Note: For webcam access in Jupyter, ensure your browser has camera permissions enabled.
ğŸ“ Project Structure
sign-language-detection/
â”‚
â”œâ”€â”€ Sign_Language_Detection.ipynb   # Main Jupyter notebook along with model
â”œâ”€â”€ requirements.txt                # Project dependencies
â”œâ”€â”€ README.md                       # Project documentation
â”œâ”€â”€ .gitignore                      # Git ignore file
â”‚
â”œâ”€â”€ dataset/                          # Dataset directory
â”‚   â”œâ”€â”€ Hello/                     # Hello gesture images
â”‚   â”œâ”€â”€ Perfect/                   # Perfect gesture images
â”‚   â”œâ”€â”€ No/                        # No gesture images
â”‚   â”œâ”€â”€ Yes/                       # Yes gesture images
â”‚   â””â”€â”€ Thanks/                    # Thanks gesture images

ğŸ§  Model Architecture
The Convolutional Neural Network (CNN) consists of:

Input Layer: Accepts preprocessed hand gesture images
Conv2D Layers: Extract spatial features from images
MaxPooling2D Layers: Reduce spatial dimensions and prevent overfitting
Flatten Layer: Convert 2D features to 1D vector
Dense Layers: Fully connected layers for classification
Dropout Layers: Regularization to prevent overfitting
Output Layer: 5 classes (Hello, Perfect, No, Yes, Thanks) with softmax activation

Training Features:

Early Stopping: Prevents overfitting by monitoring validation loss
Data Augmentation: Improves model generalization (optional)
Label Encoding: Converts gesture names to numerical labels
One-Hot Encoding: Prepares labels for categorical classification

ğŸ“Š Performance

Number of Classes: 5 (Hello, Perfect, No, Yes, Thanks)
Training Accuracy: ~95%+ (varies based on dataset)
Validation Accuracy: ~90%+ (varies based on dataset)
Real-time Detection: Works smoothly with webcam input

Note: Actual performance depends on dataset quality and size
ğŸ¯ How It Works

Data Collection: Dataset contains images of 5 hand gestures
Preprocessing: Images are resized, normalized, and prepared for training
Model Training: CNN learns to recognize patterns in each gesture
Hand Detection: MediaPipe detects hand landmarks in real-time
Feature Extraction: CNN processes the hand region
Classification: Model predicts which of the 5 gestures is shown
Display: Prediction is displayed with confidence score

ğŸ’¡ Usage Examples
Training the Model
python# In Jupyter Notebook
model.fit(X_train, y_train, 
          validation_data=(X_val, y_val),
          epochs=20, 
          batch_size=32)
Real-time Detection
python# Capture from webcam
cap = cv2.VideoCapture(0)
# Process frames and predict gestures
# Display results in real-time
ğŸ¤ Contributing
Contributions, issues, and feature requests are welcome! Feel free to check the issues page.
How to Contribute:

Fork this repository
Clone your fork: git clone https://github.com/YOUR-USERNAME/sign-language-detection.git
Create a new branch: git checkout -b feature/AmazingFeature
Make your changes
Commit your changes: git commit -m 'Add some AmazingFeature'
Push to the branch: git push origin feature/AmazingFeature
Open a Pull Request

Areas for Contribution:

ğŸ“¸ Add more gesture datasets
ğŸ¯ Improve model accuracy
ğŸŒ Add support for more sign languages
ğŸ“± Create mobile app version
ğŸ“ Improve documentation
ğŸ› Bug fixes and optimizations

ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.
This means you can:

âœ… Use commercially
âœ… Modify
âœ… Distribute
âœ… Private use

Just remember to include the original license and copyright notice!
ğŸ‘¨â€ğŸ’» Author
Kanchan - @kanchan07r
Feel free to reach out for questions or collaborations!


Issues: Found a bug? Open an issue
Discussions: Have questions? Start a discussion
GitHub: @kanchna07r

